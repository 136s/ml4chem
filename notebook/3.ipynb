{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[REINVENT4 の lock ファイル](https://github.com/MolecularAI/REINVENT4/blob/5fa8590ff03d3abf87a6543dbcf80cb530a2239f/requirements-linux-64.lock) で作成した環境で実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P. 57 予測分布\n",
    "\n",
    "予測分布 $p_\\theta(y|x)\\coloneqq\\bar{p}(y|\\sigma(g_\\theta(x)))$ (3.6)\n",
    "\n",
    "$x$ -> 関数近似器 $g_\\theta$ -> 活性化関数 $\\sigma$ -> 条件付き確率分布 $\\bar{p}(y|\\bar{y})$ -> $y$\n",
    "\n",
    "1. 関数近似器のみパラメタ $\\theta$ に依存し、活性化関数や条件付き確率分布はパラメタに依存しない\n",
    "2. 関数近似器は、出力空間 $\\mathcal{Y}$ によらず、実数または実数値ベクトルを出力する\n",
    "\n",
    "式 (3.6) の目的関数（= 損失関数）は $L(\\theta;\\mathcal{D})=\\widehat{\\mathbb{E}}_{(X,Y)\\sim\\mathcal{D}}[-\\log\\bar{p}(Y|\\sigma(g_\\theta(X)))]$ と表せる\n",
    "\n",
    "最尤推定の枠組みの中では条件付き確率分布と損失関数が 1 対 1 に対応するため、$\\bar{p}$ を損失関数とも呼べる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P.61 PyTorch を用いた予測分布の実装\n",
    "\n",
    "- 関数近似器: 線形モデル\n",
    "    - 入力空間 $\\mathcal{X}\\subseteq\\mathbb{R}^D$、パラメタ$\\theta=\\{\\mathbf{w}\\in\\mathbb{R}^D,b\\in\\mathbb{R}\\}$ のとき $g_\\theta(\\mathbf{x})=\\mathbf{w}^\\top\\mathbf{x}+b$\n",
    "    - `nn.Linear` クラス\n",
    "- 活性化関数: シグモイド関数\n",
    "    - $\\mathcal{Y}=\\{0,1\\}$ のとき $\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "- 条件付き確率分布: Bernoulli 分布\n",
    "    - $\\mathcal{Y}=\\{0,1\\}$、中間的な出力空間 $\\bar{\\mathcal{Y}}=[0,1]$、$y=1$ となる確率 $\\bar{y}$ のとき $\\bar{p}(y|\\bar{y})=\\text{Bern}(y;\\bar{y})$\n",
    "- 経験損失関数: 2 値交差エントロピー損失の期待値\n",
    "    - $L(\\theta;\\mathcal{D})=\\widehat{\\mathbb{E}}_{(X,Y)\\sim\\mathcal{D}}[-Y\\log f_\\theta(X)-(1-Y)\\log(1-f_\\theta(X))]$\n",
    "\n",
    "$\\mathcal{X}=\\mathbb{R}^5, \\mathcal{Y}=\\mathbb{R}$ に対応する線形モデルをつくる\n",
    "\n",
    "## `nn.Module` クラス\n",
    "\n",
    "数学的な関数を実装するベースクラスで、各構成要素がこれを継承している。\n",
    "ほかのモジュールの構成要素や、複雑な構造の関数近似器を定義するために使う。\n",
    "`nn.Module` クラスを継承するときは、`__init__()` と `forward()` メソッドを実装する必要がある。\n",
    "`__init__()` は関数の内部で使うパラメタ（パラメタやパラメタを持つモジュール）、`forward()` はクラスで表現したい計算を手続き的に書く。\n",
    "\n",
    "PyTorch には標準で、活性化関数と損失関数が一体になったモジュールが実装されている。\n",
    "例えば、`nn.CrossEntropyLoss` は、ソフトマックス関数と交差エントロピー損失を組み合わせたものである。\n",
    "また、`nn.BCELoss` は、2 値交差エントロピー損失とシグモイド関数を組み合わせたものである。\n",
    "教師あり学習の場合、出力空間 $\\mathcal{Y}$ が決まるとそれに応じて活性化関数と損失関数が決まるので、活性化関数と損失関数は対になる存在である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model weight: Parameter containing:\n",
      "tensor([[ 0.1441, -0.3936,  0.0156, -0.3044,  0.2281]], requires_grad=True)\n",
      "model bias: Parameter containing:\n",
      "tensor([0.3032], requires_grad=True)\n",
      "\n",
      "input: tensor([1., 1., 1., 1., 1.])\n",
      "output: tensor([-0.0070], grad_fn=<ViewBackward0>)\n",
      "\n",
      "input: tensor([[0., 1., 2., 3., 4.],\n",
      "        [5., 6., 7., 8., 9.]])\n",
      "output: tensor([[-0.0600],\n",
      "        [-1.6112]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "output: tensor([[0.4850],\n",
      "        [0.1664]], grad_fn=<SigmoidBackward0>)\n",
      "\n",
      "target: tensor([0., 1.])\tloss: 1.2283895015716553\n",
      "target: tensor([1., 0.])\tloss: 0.4528266489505768\n"
     ]
    }
   ],
   "source": [
    "# P.62 リスト3.1: 3/pred_func.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(46)\n",
    "\n",
    "# 線形モデル（入力次元 5、出力次元 1）\n",
    "m = nn.Linear(in_features=5, out_features=1)\n",
    "print(\"model weight: {}\".format(m.weight))\n",
    "print(\"model bias: {}\".format(m.bias))\n",
    "print()\n",
    "\n",
    "# 入力ベクトル\n",
    "x = torch.ones(5)\n",
    "# 出力ベクトル\n",
    "y = m(x)  # m.forward(x) と等価\n",
    "print(\"input: {}\".format(x))\n",
    "print(\"output: {}\".format(y))\n",
    "print()\n",
    "\n",
    "# 複数（2 つ）の 5 次元ベクトルをまとめた行列を入力として与える\n",
    "# 0 から 9 までの数字を 2 行 5 列の行列に変換\n",
    "X = torch.arange(10, dtype=torch.float).reshape(2, 5)\n",
    "# 2 つの入力ベクトルに対応する 2*1 の出力ベクトルを得る\n",
    "Y = m(X)\n",
    "print(\"input: {}\".format(X))\n",
    "print(\"output: {}\".format(Y))\n",
    "print()\n",
    "\n",
    "# 活性化関数（シグモイド関数）を定義して適用\n",
    "activation = nn.Sigmoid()\n",
    "Y_bar = activation(m(X))\n",
    "print(\"output: {}\".format(Y_bar))\n",
    "print()\n",
    "\n",
    "target1 = torch.tensor([0.0, 1.0])\n",
    "target2 = torch.tensor([1.0, 0.0])\n",
    "# 2 値交差エントロピー損失\n",
    "loss_func = nn.BCELoss()\n",
    "loss1 = loss_func(Y_bar.reshape(-1), target1)\n",
    "loss2 = loss_func(Y_bar.reshape(-1), target2)\n",
    "print(\"target: {}\\tloss: {}\".format(target1, loss1))\n",
    "print(\"target: {}\\tloss: {}\".format(target2, loss2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P.69 順伝播型ニューラルネットワーク (L=2) の実装\n",
    "\n",
    "データ生成に用いるパラメタ $\\mathbf{w}\\in\\mathbb{R}^2$\n",
    "\n",
    "目的変数 $y=\\sin\\left(\\mathbf{w}^\\top\\mathbf{x}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# P.70 リスト3.3: 3/fnn.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class FeedforwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(FeedforwardNeuralNetwork, self).__init__()\n",
    "        # 線形関数 → 活性化関数 (ReLU) → 線形関数 を直列 (sequential) に繋ぐ\n",
    "        self.fnn = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fnn(x)\n",
    "\n",
    "\n",
    "# サンプルサイズ 1000 の 2 次元説明変数を標準正規分布に従って生成\n",
    "torch.manual_seed(0)\n",
    "in_dim = 2\n",
    "sample_size = 1000\n",
    "X = torch.randn((sample_size, in_dim))  # (1000, 2)\n",
    "# データ生成に用いるパラメタを標準正規分布に従って生成\n",
    "w = torch.randn(in_dim)  # (2,)\n",
    "# 目的変数を生成\n",
    "y = torch.sin(X @ w).reshape(-1, 1)  # (1000, 1)\n",
    "\n",
    "# FFN モデルのインスタンスを生成（入力次元 2、中間層次元 2、出力次元 1）\n",
    "model = FeedforwardNeuralNetwork(in_dim=in_dim, hidden_dim=2)\n",
    "\n",
    "# 説明変数 X の予測値を得る\n",
    "y_pred: torch.Tensor = model.forward(X)  # (1000, 1)\n",
    "\n",
    "# ランダムな生成データと予測値の比較散布図をプロット\n",
    "vmin = min(y.min(), y_pred.min())\n",
    "vmax = max(y.max(), y_pred.max())\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(15, 6))\n",
    "im = ax1.scatter(X[:, 0], X[:, 1], c=y, cmap=\"gray\", vmin=vmin, vmax=vmax)\n",
    "ax1.set_title(\"Ground truth\")\n",
    "im = ax2.scatter(\n",
    "    X[:, 0], X[:, 1], c=y_pred.detach().numpy(), cmap=\"gray\", vmin=vmin, vmax=vmax\n",
    ")\n",
    "ax2.set_title(\"Prediction\")\n",
    "fig.colorbar(im, ax=ax2)\n",
    "plt.savefig(\"fnn.pdf\")\n",
    "plt.clf()\n",
    "\n",
    "# 散布図の各点はデータセットの各事例 (1000) に対応し、濃さが目的変数の値の大きさに対応する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P.74 誤差逆伝播法\n",
    "\n",
    "- $L$ 層ニューラルネットワーク\n",
    "- 入力: $\\mathbf{x}\\in\\mathbb{R}^D$\n",
    "- 出力: 実数値ラベル $\\mathbf{y}\\in\\mathbb{R}$\n",
    "- $l$ 層目 ($l\\in[L]$): 非線型関数 $\\mathbf{u}_l=W_l\\mathbf{z}_l$（式 (3.17)）, $\\mathbf{z}_{l+1}=\\sigma_l(\\mathbf{u}_l)$ （式 (3.18)）\n",
    "    - 入力: $\\mathbf{z}_l\\in\\mathbb{R}^{D_l}$\n",
    "    - 活性化関数: $\\sigma_l$\n",
    "    - 出力: $\\mathbf{z}_{l+1}\\in\\mathbb{R}^{D_{l+1}}$ （= $l+1$ 層目の入力）\n",
    "    - $\\mathbf{z}_0=\\mathbf{x}$\n",
    "    - $\\mathbf{z}_L=y$\n",
    "    - パラメタ: $W_l\\in\\mathbb{R}^{D_l\\times D_{l+1}}$\n",
    "        - $(i, j)\\in[D_l]\\times[D_{l+1}]$ 要素を $W_{lij}$ と表す\n",
    "- 式 (3.17) と式 (3.18) をすべての $l\\in[L]$ について合成したものを $y=f(\\mathbf{x};W)$ とする\n",
    "\n",
    "実数値ラベルを考えているため、1 事例あたりの損失関数は $\\ell(W;\\mathbf{x},y)=\\frac{1}{2}(y-f(\\mathbf{x};W))^2$ となる。この偏微分 $\\frac{\\partial \\ell(W;\\mathbf{x},y)}{\\partial W_{lij}}$ を求める。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- $l$ 層目の重み $W_l$ から $\\ell(W;\\mathbf{x},y)$ への影響（= 偏微分）を分割\n",
    "    - $W_l$ から $\\mathbf{u}_l$ への影響\n",
    "    - $\\mathbf{u}_l$ から $\\ell(W;\\mathbf{x},y)$ への影響\n",
    "        - $\\mathbf{u}_l$ から $\\mathbf{u}_{l+1}$ への影響\n",
    "        - $\\mathbf{u}_{l+1}$ から $\\ell(W;\\mathbf{x},y)$ への影響\n",
    "\n",
    "この漸化式を用いて影響を計算する\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 97.0\n",
      "grad = tensor([57., 75., 93.])\n"
     ]
    }
   ],
   "source": [
    "# P.78 リスト3.4: 3/backprop.py\n",
    "\n",
    "import torch\n",
    "\n",
    "# 2 事例からデータ\n",
    "X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])  # (2, 3)\n",
    "y = torch.tensor([1.0, 2.0])  # (2,)\n",
    "# 3 次元パラメタ w\n",
    "w = torch.tensor([1.0, 1.0, 1.0], requires_grad=True)  # (3,)\n",
    "# 損失関数（式 (3.23)）の値を計算し、この過程で計算グラフを構築\n",
    "loss = 0.5 * torch.sum((y - X @ w) ** 2)\n",
    "print(\"loss = {}\".format(loss))\n",
    "# 計算グラフをもとに誤差逆伝播法を適用して、各パラメタに関する勾配の値を計算\n",
    "loss.backward()\n",
    "# 勾配の値\n",
    "print(\"grad = {}\".format(w.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P.78 確率的勾配降下法\n",
    "\n",
    "経験値最小化問題 (P.55) を解析的に解くことができないので、勾配法などで数値的に最適解を求める必要がある。\n",
    "よく使われるのが確率的勾配降下法 stomachastic gradient descent である。\n",
    "\n",
    "最急降下法 gradient descent は、目的関数の最小化を行うためのアルゴリズム。\n",
    "例えば、$\\hat\\theta\\in\\argmin_{\\theta\\in\\Theta}L(\\theta;\\mathcal{D})$（式 (3.5)）の最適化問題の場合、初期値 $\\theta^{(0)}\\in\\Theta$ から初めて $k=0,1,\\ldots,K-1$ に対して $\\theta^{(k+1)}\\leftarrow\\theta^{(k)}-\\alpha_k\\frac{\\partial L(\\theta^{(k)};\\mathcal{D})}{\\partial\\theta}$ と更新する。\n",
    "最後の $\\theta^{(K)}$ が $\\hat\\theta$ の最適解に近似する。\n",
    "$\\alpha_k\\in\\mathbb{R}_{>0}$ は学習率 learning rate である。\n",
    "繰り返し毎に目的関数の勾配 $\\frac{\\partial L(\\theta;\\mathcal{D})}{\\partial\\theta}=\\frac{1}{|\\mathcal{D}|}\\sum_{(x,y)\\in\\mathcal{D}}\\frac{\\partial\\ell(\\theta;x,y)}{\\partial\\theta}$ を計算する必要があり、データセットのサイズが大きくなるほど計算時間が長くなる。\n",
    "\n",
    "確率的勾配降下法は真の勾配の代わりに、ランダムに選んだデータ点に対する勾配の不偏推定量を用いる。\n",
    "データセットからランダムに取得した $(X,Y)\\sim\\mathcal{D}$ について $\\frac{\\partial\\ell(\\theta;X,Y)}{\\partial\\theta}$ は真の不偏推定量となる。\n",
    "\n",
    "\n",
    "リスト 3.3 で定義したモデルと人工データを用いてリスト 3.4 と同じく `backward()` メソッドで誤差逆伝播法に基づいて勾配を推定する。\n",
    "ただし、ここで計算しているのは訓練データに対する損失関数の値で、未知データへの予測性能は推定していない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 100,\t\tloss: 0.19409441947937012\n",
      "step: 200,\t\tloss: 0.02062988467514515\n",
      "step: 300,\t\tloss: 0.004414909519255161\n",
      "step: 400,\t\tloss: 0.003762062406167388\n",
      "step: 500,\t\tloss: 0.005277620628476143\n",
      "step: 600,\t\tloss: 0.003880226518958807\n",
      "step: 700,\t\tloss: 0.004136079456657171\n",
      "step: 800,\t\tloss: 0.0030578880105167627\n",
      "step: 900,\t\tloss: 0.011349925771355629\n",
      "step: 1000,\t\tloss: 0.0037335397209972143\n",
      "step: 1100,\t\tloss: 0.0012251554289832711\n",
      "step: 1200,\t\tloss: 0.0016262279823422432\n",
      "step: 1300,\t\tloss: 0.003423119429498911\n",
      "step: 1400,\t\tloss: 0.002165834419429302\n",
      "step: 1500,\t\tloss: 0.0012058628490194678\n",
      "step: 1600,\t\tloss: 0.0014684993075206876\n",
      "step: 1700,\t\tloss: 0.006261782720685005\n",
      "step: 1800,\t\tloss: 0.0019077459583058953\n",
      "step: 1900,\t\tloss: 0.001747734029777348\n",
      "step: 2000,\t\tloss: 0.002932086819782853\n",
      "step: 2100,\t\tloss: 0.0019618840888142586\n",
      "step: 2200,\t\tloss: 0.0022173600737005472\n",
      "step: 2300,\t\tloss: 0.002852068282663822\n",
      "step: 2400,\t\tloss: 0.0006354739889502525\n",
      "step: 2500,\t\tloss: 0.0023419209755957127\n",
      "step: 2600,\t\tloss: 0.0029883887618780136\n",
      "step: 2700,\t\tloss: 0.0033628521487116814\n",
      "step: 2800,\t\tloss: 0.008427410386502743\n",
      "step: 2900,\t\tloss: 0.005515712313354015\n",
      "step: 3000,\t\tloss: 0.006641096901148558\n",
      "step: 3100,\t\tloss: 0.004668134730309248\n",
      "step: 3200,\t\tloss: 0.005263671278953552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# P.82 リスト3.6: 3/sgd.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "# リスト3.3 と同じ\n",
    "class FeedforwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(FeedforwardNeuralNetwork, self).__init__()\n",
    "        # 線形関数 → 活性化関数 (ReLU) → 線形関数 を直列 (sequential) に繋ぐ\n",
    "        self.fnn = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fnn(x)\n",
    "\n",
    "\n",
    "# サンプルサイズ 1000 の 2 次元説明変数を標準正規分布に従って生成（リスト3.3 と同じ）\n",
    "torch.manual_seed(0)\n",
    "sample_size = 1000\n",
    "in_dim = 2\n",
    "X = torch.randn((sample_size, in_dim))  # (1000, 2)\n",
    "# データ生成に用いるパラメタを標準正規分布に従って生成\n",
    "w = torch.randn(in_dim)  # (2,)\n",
    "# 目的変数を生成\n",
    "y = torch.sin(X @ w).reshape(-1, 1)  # (1000, 1)\n",
    "\n",
    "# 生成したデータセットを PyTorch のデータセットに変換\n",
    "dataset = TensorDataset(X, y)\n",
    "# シャッフルありでミニバッチサイズ 32 のデータローダを生成\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# FFN モデルのインスタンスを生成（入力次元 2、中間層次元 2、出力次元 1）（リスト3.3 と同じ）\n",
    "model = FeedforwardNeuralNetwork(in_dim=in_dim, hidden_dim=10)\n",
    "\n",
    "# 損失関数のインスタンスを作成（実数値ラベルを予測するための損失関数として平均二乗誤差を用いる）\n",
    "loss_func = nn.MSELoss()\n",
    "# 最適化手法のインスタンスを作成（モデルパラメタの最適化に確率的勾配降下法 (SGD) を用いる）\n",
    "# ここでは、確率的勾配降下法の中で Adaptive Moment Estimation を用いる\n",
    "# 学習率 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "n_step = 0\n",
    "loss_list = []\n",
    "for each_epoch in range(100):\n",
    "    for each_X, each_y in dataloader:  # 32 事例ずつランダムにデータを呼び出す\n",
    "        # 誤差逆伝播法、確率的勾配降下法の 1 ステップを実行\n",
    "        # モデルによる予測値\n",
    "        each_pred = model.forward(each_X)\n",
    "        # モデルによる予測値と真値から損失を計算\n",
    "        loss: torch.Tensor = loss_func(each_pred, each_y)\n",
    "        # backward() 実行の度に optimizer に勾配が累積されるため、勾配を 0 にリセット\n",
    "        optimizer.zero_grad()\n",
    "        # パラメータに関する損失の勾配を計算\n",
    "        loss.backward()\n",
    "        # 蓄積した勾配を元にパラメタを更新\n",
    "        optimizer.step()\n",
    "\n",
    "        n_step += 1\n",
    "        if n_step % 100 == 0:\n",
    "            print(\"step: {},\\t\\tloss: {}\".format(n_step, loss.item()))\n",
    "            loss_list.append((n_step, loss.item()))\n",
    "\n",
    "\n",
    "# 説明変数 X の予測値を得る（リスト3.3 と同じ）\n",
    "y_pred: torch.Tensor = model.forward(X)\n",
    "\n",
    "# ランダムな生成データと予測値の比較散布図をプロット（リスト3.3 と同じ）\n",
    "vmin = min(y.min(), y_pred.min())\n",
    "vmax = max(y.max(), y_pred.max())\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(15, 6))\n",
    "im = ax1.scatter(X[:, 0], X[:, 1], c=y, cmap=\"gray\", vmin=vmin, vmax=vmax)\n",
    "ax1.set_title(\"Ground truth\")\n",
    "im = ax2.scatter(\n",
    "    X[:, 0], X[:, 1], c=y_pred.detach().numpy(), cmap=\"gray\", vmin=vmin, vmax=vmax\n",
    ")\n",
    "ax2.set_title(\"Prediction\")\n",
    "fig.colorbar(im, ax=ax2)\n",
    "plt.savefig(\"sgd.pdf\")\n",
    "plt.clf()\n",
    "\n",
    "# 学習曲線 learning curve をプロット\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(*list(zip(*loss_list)))\n",
    "ax.set_title(\"Loss\")\n",
    "ax.set_xlabel(\"# of updates\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "plt.savefig(\"sgd_loss.pdf\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth: [0 0 1 1 1]\n",
      "pred: [0 0 1 1 0]\n",
      "accuracy: 0.8\n",
      "precision: 1.0\n",
      "recall: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# P.101 リスト3.7: 3/metrics.py\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([0, 0, 1, 1, 1])\n",
    "y_pred = np.array([0, 0, 1, 1, 0])\n",
    "print(\"truth: {}\".format(y_true))\n",
    "print(\"pred: {}\".format(y_pred))\n",
    "print(\"accuracy: {}\".format(accuracy_score(y_true, y_pred)))\n",
    "print(\"precision: {}\".format(precision_score(y_true, y_pred)))\n",
    "print(\"recall: {}\".format(recall_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P. 105 ドロップアウト\n",
    "\n",
    "過剰適合を防ぐために、モデルの大きさを制限する正則化という手法がある。\n",
    "繁用される正規化手法には正則化項、学習の早期停止、ドロップアウトなどがあり、ここではドロップアウトについて説明する。\n",
    "\n",
    "ドロップアウト dropout は、ニューラルネットワークの学習中にランダムにニューロンを間引く。\n",
    "$p\\in[0,1]$ をハイパーパラメータとして、$\\text{Dropout}_p:\\mathbf{R}^D\\rightarrow\\mathbf{R}^D$ は、次のように定義される。\n",
    "\n",
    "$\\text{Dropout}_p(\\mathbf{x})_d=\\begin{cases}\\frac{1}{1-p}x_d&{\\text{（確率}1-p\\text{）}}\\\\0&{\\text{（確率}p\\text{）}}\\end{cases}(d=1,2,\\dots,D)$ (3.36)\n",
    "\n",
    "ドロップアウトによって、指数的に存在する初めのネットワーク構造の部分的なニューラルネットワークを表現できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * train mode\n",
      "tensor([2., 2., 0., 2., 0., 2., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 2., 2., 0., 0., 0., 0., 0.])\n",
      "tensor([2., 2., 0., 2., 0., 2., 0., 2., 0., 2.])\n",
      " * evaluation mode\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# P.109 リスト3.9: 3/dropout.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(43)\n",
    "dropout_layer = nn.Dropout(p=0.5)\n",
    "input_tensor = torch.ones(10)\n",
    "\n",
    "print(\" * train mode\")\n",
    "dropout_layer.train()  # 訓練モードに切り替え（ドロップアウト有効）\n",
    "for _ in range(3):\n",
    "    print(dropout_layer(input_tensor))\n",
    "\n",
    "print(\" * evaluation mode\")\n",
    "dropout_layer.eval()  # 評価モードに切り替え（ドロップアウト無効）\n",
    "for _ in range(3):\n",
    "    print(dropout_layer(input_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P. 115 `deepchem` を用いた MoleculeNet のデータ取得とその図示\n",
    "\n",
    "[MoleculeNet](https://moleculenet.org/) は、分子構造からのその物性値を予測する問題を集めたベンチマークであり、`deepchem` で提供される。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/yuki/miniconda3/envs/reinvent4/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: \n",
      "       X_shape    y_shape\n",
      "0  (1210, 208)  (1210, 1)\n",
      "val_set: \n",
      "      X_shape   y_shape\n",
      "0  (151, 208)  (151, 1)\n",
      "test_set: \n",
      "      X_shape   y_shape\n",
      "0  (152, 208)  (152, 1)\n",
      "y_mean, y_std = 3.1475281517968074e-15, 0.9999999999999999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# P.116 リスト3.11: 3/deepchem_dataload.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import deepchem as dc  # v2.7.1\n",
    "\n",
    "# 特徴量抽出器（構造式から特徴量ベクトルに変換する）として、RDKit の記述子（208 個）を用いる\n",
    "featurizer = dc.feat.RDKitDescriptors()\n",
    "# Molenet から、回帰問題設定の BACE データセット（BACE-1 阻害薬の構造式とその IC50 のセット）を読み込む\n",
    "# 引数の初期値で、分子骨格を基にデータセットを分割し、目的変数を標準化している\n",
    "# tasks: データセットに対して定義されているタスクのリスト（ここでは pIC50 のみ）\n",
    "# datasets: 訓練データ、検証用データ、テストデータのタプル\n",
    "# transformers: データセットに対して適用される変換器のリスト\n",
    "tasks, datasets, transformers = dc.molnet.load_bace_regression(featurizer)\n",
    "\n",
    "# 1210, 151, 152 事例の訓練、検証、テストセットに分割\n",
    "train_set, val_set, test_set = datasets\n",
    "# 訓練データの、サンプルサイズ × 特徴量ベクトルの次元\n",
    "print(\"train_set: \\n{}\".format(train_set.metadata_df.iloc[:, 5:7]))\n",
    "# 検証データの、サンプルサイズ × 特徴量ベクトルの次元\n",
    "print(\"val_set: \\n{}\".format(val_set.metadata_df.iloc[:, 5:7]))\n",
    "# テストデータの、サンプルサイズ × 特徴量ベクトルの次元\n",
    "print(\"test_set: \\n{}\".format(test_set.metadata_df.iloc[:, 5:7]))\n",
    "# 訓練セットの目的変数の分布\n",
    "print(\"y_mean, y_std = {}, {}\".format(np.mean(train_set.y), np.std(train_set.y)))\n",
    "# 訓練データの目的変数のヒストグラム → 標準正規分布となっていることを確認\n",
    "plt.hist(train_set.y, bins=int(np.sqrt(len(train_set.y))))\n",
    "plt.xlabel(\"Normalized pIC50\")\n",
    "plt.savefig(\"bace_y_train.pdf\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0,\t\ttrain loss: 326.12925409994835\n",
      "step: 0,\t\tval loss: 338.96144518315396\n",
      "step: 100,\t\ttrain loss: 1.2605398414548763\n",
      "step: 100,\t\tval loss: 0.6173935252309635\n",
      "step: 200,\t\ttrain loss: 0.7197361906697928\n",
      "step: 200,\t\tval loss: 0.6936031745759067\n",
      "step: 300,\t\ttrain loss: 0.5462111165700865\n",
      "step: 300,\t\tval loss: 0.4843808483603774\n",
      "step: 400,\t\ttrain loss: 0.6172513197276218\n",
      "step: 400,\t\tval loss: 0.8962339439139461\n",
      "step: 500,\t\ttrain loss: 0.43083953187485374\n",
      "step: 500,\t\tval loss: 0.424688269760435\n",
      "step: 600,\t\ttrain loss: 0.40749028497491\n",
      "step: 600,\t\tval loss: 0.5425087947719145\n",
      "step: 700,\t\ttrain loss: 0.44993086018838174\n",
      "step: 700,\t\tval loss: 0.32921599394438283\n",
      "step: 800,\t\ttrain loss: 0.3829679895038447\n",
      "step: 800,\t\tval loss: 0.4277042742596557\n",
      "step: 900,\t\ttrain loss: 0.5897707332264294\n",
      "step: 900,\t\tval loss: 0.3912609302444963\n",
      "step: 1000,\t\ttrain loss: 0.3515798643600842\n",
      "step: 1000,\t\tval loss: 0.3981382673149867\n",
      "step: 1100,\t\ttrain loss: 0.37781591415405275\n",
      "step: 1100,\t\tval loss: 0.37029400250769606\n",
      "test_loss: 0.6025878512169704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# P.118 リスト3.13: 3/deepchem_main.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import deepchem as dc\n",
    "\n",
    "# from fnn import FeedforwardNeuralNetwork\n",
    "\n",
    "\n",
    "# リスト3.3 と同じ\n",
    "class FeedforwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(FeedforwardNeuralNetwork, self).__init__()\n",
    "        # 線形関数 → 活性化関数 (ReLU) → 線形関数 を直列 (sequential) に繋ぐ\n",
    "        self.fnn = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fnn(x)\n",
    "\n",
    "\n",
    "torch.manual_seed(43)\n",
    "\n",
    "\n",
    "def loss_evaluator(dataloader, model, loss_func):\n",
    "    # データローダ、モデル、損失関数が与えられた下で、1 事例あたりの平均的な損失の値と、予測値のリスト、実測値のリストを返す\n",
    "    sample_size = len(dataloader.dataset)\n",
    "    pred_list = []\n",
    "    true_list = []\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for each_X, each_y in dataloader:\n",
    "            each_pred = model.forward(each_X)\n",
    "            pred_list.extend(each_pred.tolist())\n",
    "            true_list.extend(each_y.tolist())\n",
    "            loss += loss_func(each_pred, each_y).item()\n",
    "    return loss / sample_size, pred_list, true_list\n",
    "\n",
    "\n",
    "# 特徴量抽出器（構造式から特徴量ベクトルに変換する）として、RDKit の記述子（208 個）を用いる（リスト3.11 と同じ）\n",
    "featurizer = dc.feat.RDKitDescriptors()\n",
    "# Molenet から、回帰問題設定の BACE データセット（BACE-1 阻害薬の構造式とその IC50 のセット）を読み込む（リスト3.11 と同じ）\n",
    "# 引数の初期値で、分子骨格を基にデータセットを分割し、目的変数を標準化している\n",
    "# tasks: データセットに対して定義されているタスクのリスト（ここでは pIC50 のみ）\n",
    "# datasets: 訓練データ、検証用データ、テストデータのタプル\n",
    "# transformers: データセットに対して適用される変換器のリスト\n",
    "tasks, datasets, transformers = dc.molnet.load_bace_regression(featurizer)\n",
    "\n",
    "# 1210, 151, 152 事例の訓練、検証、テストセットに分割（リスト3.11 と同じ）\n",
    "train_set, val_set, test_set = datasets\n",
    "\n",
    "# シャッフルありでミニバッチサイズ 32 で訓練データののデータローダを生成\n",
    "train_dataloader = DataLoader(\n",
    "    TensorDataset(torch.FloatTensor(train_set.X), torch.FloatTensor(train_set.y)),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "# シャッフルありでミニバッチサイズ 32 で検証データののデータローダを生成\n",
    "val_dataloader = DataLoader(\n",
    "    TensorDataset(torch.FloatTensor(val_set.X), torch.FloatTensor(val_set.y)),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "# シャッフルありでミニバッチサイズ 32 でテストデータののデータローダを生成\n",
    "test_dataloader = DataLoader(\n",
    "    TensorDataset(torch.FloatTensor(test_set.X), torch.FloatTensor(test_set.y)),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# FFN モデルのインスタンスを生成（入力次元 208、中間層次元、出力次元 1）\n",
    "model = FeedforwardNeuralNetwork(in_dim=train_set.X.shape[1], hidden_dim=32)\n",
    "# 損失関数のインスタンスを作成\n",
    "loss_func = nn.MSELoss(reduction=\"sum\")\n",
    "# 最適化手法のインスタンスを作成（モデルパラメタの最適化に確率的勾配降下法 (SGD) を用いる）\n",
    "# weight_decay で L2 正則化項を追加して、過剰適合を軽減している\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "n_step = 0\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "for each_epoch in range(30):\n",
    "    for each_X, each_y in train_dataloader:\n",
    "        if n_step % 10 == 0:\n",
    "            # 訓練損失を計算\n",
    "            train_loss, _, _ = loss_evaluator(train_dataloader, model, loss_func)\n",
    "            # 検証損失を計算\n",
    "            val_loss, _, _ = loss_evaluator(val_dataloader, model, loss_func)\n",
    "            if n_step % 100 == 0:\n",
    "                print(\"step: {},\\t\\ttrain loss: {}\".format(n_step, train_loss))\n",
    "                print(\"step: {},\\t\\tval loss: {}\".format(n_step, val_loss))\n",
    "            train_loss_list.append((n_step, train_loss))\n",
    "            val_loss_list.append((n_step, val_loss))\n",
    "\n",
    "        each_pred = model.forward(each_X)\n",
    "        loss = loss_func(each_pred, each_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        n_step += 1\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(*list(zip(*train_loss_list)), marker=\"+\")\n",
    "ax.plot(*list(zip(*val_loss_list)), marker=\".\")\n",
    "ax.set_title(\"Learning curve\")\n",
    "ax.set_xlabel(\"# of updates\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_yscale(\"log\")\n",
    "plt.savefig(\"bace_loss.pdf\")\n",
    "plt.clf()\n",
    "\n",
    "# テスト損失、予測値のリスト、実測値のリストを得る\n",
    "test_loss, pred_list, true_list = loss_evaluator(train_dataloader, model, loss_func)\n",
    "print(\"test_loss: {}\".format(test_loss))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.scatter(pred_list, true_list)\n",
    "ax.set_title(\"Test loss = {}\".format(test_loss))\n",
    "ax.set_xlabel(\"Predicted normalized pIC50\")\n",
    "ax.set_ylabel(\"True normalized pIC50\")\n",
    "plt.savefig(\"bace_scatter.pdf\")\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinvent4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
