{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1273104, 102])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from smiles_vocab import SmilesVocabulary\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch.mps.manual_seed(330252033)\n",
    "\n",
    "smiles_vocab = SmilesVocabulary()\n",
    "# 訓練データと検証データの整数系列を作成\n",
    "train_tensor: torch.Tensor = smiles_vocab.batch_update_from_file(\"train.smi\").to(device)\n",
    "train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 102]), torch.Size([256, 102]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "# シャッフルありでバッチモードで訓練データの DataLoader を作成\n",
    "train_dataset = TensorDataset(torch.flip(train_tensor, dims=[1]), train_tensor)\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "train_data_loader_iter = train_data_loader.__iter__()\n",
    "each_train_batch = train_data_loader_iter.__next__()\n",
    "\n",
    "# バッチごとに学習\n",
    "in_seq: torch.Tensor = each_train_batch[0].to(device)\n",
    "out_seq: torch.Tensor = each_train_batch[1].to(device)\n",
    "in_seq.shape, out_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `SmilesVAE.loss()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SmilesVAE.forward()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SmilesVAE.encode()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size=41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([41, 256])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "emb_dim = 256\n",
    "vocab = smiles_vocab\n",
    "vocab_size = len(vocab.char_list)\n",
    "print(f\"{vocab_size=}\")\n",
    "\n",
    "# 埋め込みベクトル\n",
    "embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=vocab.pad_idx, device=device)\n",
    "embedding.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 102, 256])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in_seq (batch_size, seq_len) を埋め込みベクトルの行列に変換\n",
    "in_seq_emb: torch.Tensor = embedding(in_seq)\n",
    "in_seq_emb.shape  # バッチサイズ * 系列長 * 隠れ状態の次元数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `self.encoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 102, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_params = {\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_layers\": 1,\n",
    "    \"bidirectional\": False,\n",
    "    \"dropout\": 0.0,\n",
    "}\n",
    "\n",
    "# 埋め込みベクトルの系列をエンコーダに入力\n",
    "# 隠れ状態の系列 out_seq: サンプルサイズ * 系列長 * 隠れ状態の次元\n",
    "# 最終隠れ状態 (h, c)\n",
    "encoder = nn.LSTM(emb_dim, batch_first=True, **encoder_params, device=device)\n",
    "encoder_out_seq, (h, c) = encoder(in_seq_emb)\n",
    "encoder_out_seq.shape  # バッチサイズ * 系列長 * 隠れ状態の次元数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `self.encoder2out`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 512]), torch.Size([256, 256]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_dim = 512\n",
    "each_out_dim = 256\n",
    "\n",
    "# エンコーダの LSTM の出力を変換する多層ニューラルネットワーク\n",
    "encoder2out: nn.Sequential = nn.Sequential()\n",
    "encoder2out.append(nn.Linear(in_dim, each_out_dim, device=device))\n",
    "encoder2out.append(nn.Sigmoid())\n",
    "in_dim = each_out_dim\n",
    "\n",
    "# 末尾の隠れ状態は、入力系列すべてを反映した隠れ状態であり、これを使ってエンコーダの出力を作る\n",
    "last_out: torch.Tensor = encoder_out_seq[:, -1, :]\n",
    "out: torch.Tensor = encoder2out(last_out)\n",
    "last_out.shape, out.shape  # バッチサイズ * 隠れ状態の次元数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `self.encoder_out2mu`, `self.encoder_out2logvar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 64]), torch.Size([256, 64]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim = 64\n",
    "\n",
    "# self.encoder2out の出力を潜在空間上の正規分布の平均に変換する線形モデル\n",
    "encoder_out2mu = nn.Linear(in_dim, latent_dim, device=device)\n",
    "\n",
    "# self.encoder2out の出力を潜在空間上の正規分布の分散共分散行列の対角成分に変換する線形モデル\n",
    "encoder_out2logvar = nn.Linear(in_dim, latent_dim, device=device)\n",
    "\n",
    "# 潜在空間上の正規分布の平均と分散共分散行列をつくり、エンコーダの出力とする\n",
    "mu: torch.Tensor = encoder_out2mu(out)\n",
    "logvar: torch.Tensor = encoder_out2logvar(out)\n",
    "mu.shape, logvar.shape  # バッチサイズ * 潜在空間の次元数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SmilesVAE.reparam()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 潜在空間上の正規分布の分散共分散行列の対角成分の対数から標準偏差を計算\n",
    "std = torch.exp(0.5 * logvar)\n",
    "# 標準偏差を何倍にするかを正規分布からランダムサンプリング\n",
    "eps = torch.randn_like(std)\n",
    "z = mu + std * eps\n",
    "z.shape  # バッチサイズ * 潜在空間の次元数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SmilesVAE.decode()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `self.latent2dech`, `self.latent2decc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=64, out_features=512, bias=True),\n",
       " Linear(in_features=64, out_features=512, bias=True))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_params = {\"hidden_size\": 512, \"num_layers\": 1, \"dropout\": 0.0}\n",
    "# 潜在ベクトルを、デコーダの LSTM の隠れ状態に変換するモデル\n",
    "latent2dech: nn.Linear = nn.Linear(\n",
    "    latent_dim, decoder_params[\"hidden_size\"] * decoder_params[\"num_layers\"]\n",
    ").to(device)\n",
    "# 潜在ベクトルを、デコーダの LSTM の細胞状態に変換するモデル\n",
    "latent2decc: nn.Linear = nn.Linear(\n",
    "    latent_dim, decoder_params[\"hidden_size\"] * decoder_params[\"num_layers\"]\n",
    ").to(device)\n",
    "latent2dech, latent2decc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `self.decoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(256, 512, batch_first=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# デコーダ\n",
    "decoder: nn.LSTM = nn.LSTM(\n",
    "    emb_dim, batch_first=True, bidirectional=False, **decoder_params, device=device\n",
    ")\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_unstructured.shape=torch.Size([256, 512]), c_unstructured.shape=torch.Size([256, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256, 512]), torch.Size([1, 256, 512]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size: int = z.shape[0]  # 自明なのでスキップ\n",
    "\n",
    "# デコードに用いる LSTM の隠れ状態 h と細胞状態 c を潜在ベクトルから作成\n",
    "h_unstructured: torch.Tensor = latent2dech(z)\n",
    "c_unstructured: torch.Tensor = latent2decc(z)\n",
    "\n",
    "print(f\"{h_unstructured.shape=}, {c_unstructured.shape=}\")\n",
    "\n",
    "h: torch.Tensor = torch.stack(\n",
    "    [\n",
    "        h_unstructured[:, each_idx : each_idx + decoder.hidden_size]\n",
    "        for each_idx in range(0, h_unstructured.shape[1], decoder.hidden_size)\n",
    "    ]\n",
    ")\n",
    "c: torch.Tensor = torch.stack(\n",
    "    [\n",
    "        c_unstructured[:, each_idx : each_idx + decoder.hidden_size]\n",
    "        for each_idx in range(0, c_unstructured.shape[1], decoder.hidden_size)\n",
    "    ]\n",
    ")\n",
    "\n",
    "h.shape, c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `self.decoder2vocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=41, bias=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# デコーダの出力を、アルファベット空間上のロジットベクトルに変換するモデル\n",
    "decoder2vocab: nn.Linear = nn.Linear(\n",
    "    decoder_params[\"hidden_size\"], vocab_size, device=device\n",
    ")\n",
    "decoder2vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 101, 41]), torch.Size([255, 102]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正解の SMILES 系列がある場合は、正解の SMILES 系列をデコードして対数尤度を返す\n",
    "# 埋め込みベクトルの系列に変換\n",
    "out_seq_emb: torch.Tensor = embedding(out_seq)\n",
    "out_seq_emb_out, _ = decoder(out_seq_emb, (h, c))\n",
    "# 対数尤度（バッチサイズ * 系列長 * アルファベット数）を計算\n",
    "out_seq_vocab_logit: torch.Tensor = decoder2vocab(out_seq_emb_out)\n",
    "# 損失関数の計算に使われるため、系列長を 1 短くしている\n",
    "out_seq_logit, _ = out_seq_vocab_logit[:, :-1], out_seq[:-1]\n",
    "\n",
    "out_seq_logit.shape, _.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `SmilesVAE.loss()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `self.loss_func`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数\n",
    "loss_func: nn.CrossEntropyLoss = nn.CrossEntropyLoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 101])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交差エントロピー損失を計算\n",
    "neg_likelihood: torch.Tensor = loss_func(out_seq_logit.transpose(1, 2), out_seq[:, 1:])\n",
    "neg_likelihood.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(376.1754, device='mps:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# バッチごとに損失を合計し、その平均を取る\n",
    "neg_likelihood = neg_likelihood.sum(dim=1).mean()\n",
    "neg_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4408, device='mps:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KL 情報量を計算\n",
    "kl_div: torch.Tensor = (\n",
    "    -0.5 * (1.0 + logvar - mu**2 - torch.exp(logvar)).sum(dim=1).mean()\n",
    ")\n",
    "kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379.61627197265625"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = 1.0\n",
    "\n",
    "# β-VAE のため、KL 情報量に β を乗じている\n",
    "each_loss = (neg_likelihood + beta * kl_div)\n",
    "each_loss.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinvent4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
